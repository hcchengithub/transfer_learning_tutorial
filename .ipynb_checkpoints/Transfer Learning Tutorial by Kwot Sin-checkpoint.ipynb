{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Transfer Learning in TensorFlow](https://kwotsin.github.io/tech/2017/02/11/transfer-learning.html)\n",
    "# Using a Pre-trained Inception-Resnet-V2 Model  \n",
    "# by Kwot Sin\n",
    "\n",
    "one thing that I find really useful in using TensorFlow-slim over other deep learning libraries is the ready access to the best pretrained models offered by Google.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我看 Kwot Sin 這篇 blog 文說明的是這個 source code 檔:<br>\n",
    "`%USERPROFILE%\\Documents\\GitHub\\ML\\transfer_learning_tutorial\\train_flowers.py`\n",
    "--> 跑跑看。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These modules no problem I am sure\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "# done ! as anticipated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These moduels , I'd like to impor them one after one\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.platform import tf_logging as logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inception_preprocessing\n",
    "# 直接完成，可能我先前玩過 inception，已經 install 好了？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "# 直接完成，幾乎沒花時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well all done very smoothly. Now I'd like to check them out by using peforth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK .s\n",
      "empty\n",
      "\n",
      "OK words\n",
      "code end-code \\ // <selftest> </selftest> bye /// immediate stop compyle trim indent -indent <py> </py> </pyV> words . cr help interpret-only compile-only literal reveal privacy (create) : ; ( BL CR word ' , [compile] py: py> py:~ py>~ 0branch here! here swap ! @ ? >r r> r@ drop dup over 0< + * - / 1+ 2+ 1- 2- compile if then compiling char last version execute cls private nonprivate (space) exit ret rescan-word-hash (') branch bool and or not (forget) AND OR NOT XOR true false \"\" [] {} none >> << 0= 0> 0<> 0<= 0>= = == > < != >= <= abs max min doVar doNext depth pick roll space [ ] colon-word create (marker) marker next abort alias <> public nip rot -rot 2drop 2dup invert negate within ['] allot for begin until again ahead never repeat aft else while ?stop ?dup variable +! chars spaces .( .\" .' s\" s' s` does> count accept <accept> nop </accept> \u0004 refill [else] [if] [then] (::) (:>) :: :> ::~ :>~ \"msg\"abort abort\" \"msg\"?abort ?abort\" '<text> (<text>) <text> </text> <comment> </comment> (constant) constant value to tib. >t t@ t> [begin] [again] [until] [for] [next] __main__ import modules int float drops dropall char>ASCII ASCII>char ASCII .s (*debug*) *debug* readTextFile writeTextFile tib.insert dictate sinclude include break-include type obj>keys obj2dict stringify toString .literal .function (dump) dump dump2ret d (see) see slice screen-buffer display-off display-on WshShell inport harry_port OK dir keys (pyclude) pyclude .members .source dos cd --- locals globls \n",
      "OK locals dir . cr\n",
      "['__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']\n",
      "OK globals dir . cr\n",
      "\n",
      "Error! globals unknown.\n",
      "OK globls dir . cr\n",
      "['__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']\n",
      "OK globls :> keys() . cr\n",
      "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', '_', '__', '___', '_i', '_ii', '_iii', '_i1', 'peforth', '_i2', '_i3', '_i4', '_i5', 'os', 'time', 'tf', '_i6', 'get_or_create_global_step', '_i7', 'logging', '_i8', 'inception_preprocessing', '_i9', 'inception_resnet_v2', 'inception_resnet_v2_arg_scope', '_i10', 'slim', '_i11'])\n",
      "OK locals :> keys() . cr\n",
      "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', '_', '__', '___', '_i', '_ii', '_iii', '_i1', 'peforth', '_i2', '_i3', '_i4', '_i5', 'os', 'time', 'tf', '_i6', 'get_or_create_global_step', '_i7', 'logging', '_i8', 'inception_preprocessing', '_i9', 'inception_resnet_v2', 'inception_resnet_v2_arg_scope', '_i10', 'slim', '_i11'])\n",
      "OK __main__ :> slim dir . cr\n",
      "['DataFrameColumn', 'GDN', 'OPTIMIZER_CLS_NAMES', 'OPTIMIZER_SUMMARIES', 'ProblemType', 'SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY', 'VariableDeviceChooser', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'abc', 'absolute_import', 'adaptive_clipping_fn', 'add_arg_scope', 'add_model_variable', 'apply_regularization', 'arg_scope', 'arg_scoped_arguments', 'array_ops', 'assert_global_step', 'assert_or_get_global_step', 'assign_from_checkpoint', 'assign_from_checkpoint_fn', 'assign_from_values', 'assign_from_values_fn', 'avg_pool2d', 'avg_pool3d', 'batch_norm', 'bias_add', 'binary_svm_target', 'bow_encoder', 'bucketization_op', 'bucketize', 'bucketized_column', 'check_feature_columns', 'checkpoint_utils', 'clip_ops', 'collections', 'constant_op', 'contrib_framework', 'contrib_sparse_ops', 'contrib_variables', 'control_flow_ops', 'conv2d', 'conv2d_in_plane', 'conv2d_transpose', 'conv3d', 'conv3d_transpose', 'convolution', 'convolution2d', 'convolution2d_in_plane', 'convolution2d_transpose', 'convolution3d', 'convolution3d_transpose', 'create_feature_spec_for_parsing', 'create_global_step', 'crossed_column', 'data_decoder', 'data_provider', 'dataset', 'dataset_data_provider', 'deprecated', 'deprecated_arg_values', 'deprecation', 'division', 'dropout', 'dtypes', 'elu', 'embed_sequence', 'embedding_column', 'embedding_lookup_sparse_with_distributed_aggregation', 'embedding_lookup_unique', 'embedding_ops', 'encoders', 'evaluation', 'experimental', 'fc', 'fc_core', 'feature_column', 'feature_column_ops', 'filter_variables', 'flatten', 'fully_connected', 'functools', 'gdn', 'gen_sparse_feature_cross_op', 'get_default_binary_metrics_for_eval', 'get_global_step', 'get_local_variables', 'get_model_variables', 'get_or_create_global_step', 'get_trainable_variables', 'get_unique_variable', 'get_variable_full_name', 'get_variables', 'get_variables_by_name', 'get_variables_by_suffix', 'get_variables_to_restore', 'has_arg_scope', 'infer_real_valued_columns', 'init_ops', 'initializers', 'input_from_feature_columns', 'instance_norm', 'joint_weighted_sum_from_feature_columns', 'l1_l2_regularizer', 'l1_regularizer', 'l2_regularizer', 'layer_norm', 'layers', 'learning', 'legacy_fully_connected', 'legacy_linear', 'legacy_relu', 'linear', 'loader', 'local_variable', 'logging', 'lookup', 'loss_ops', 'losses', 'make_all', 'make_place_holder_tensors_for_base_features', 'math', 'math_ops', 'max_pool2d', 'max_pool3d', 'maxout', 'metric_ops', 'metrics', 'model_analyzer', 'model_variable', 'moving_averages', 'multi_class_target', 'nest', 'nn', 'nn_ops', 'normalization', 'one_hot_column', 'one_hot_encoding', 'ops', 'optimize_loss', 'optimizer_', 'optimizers', 'parallel_reader', 'parse_feature_columns_from_examples', 'parse_feature_columns_from_sequence_examples', 'parsing_ops', 'pool', 'prefetch_queue', 'print_function', 'python', 'queues', 'random_ops', 'real_valued_column', 'regression_target', 'regularizers', 'relu', 'relu6', 'repeat', 'resource_loader', 'resource_variable_ops', 'safe_embedding_lookup_sparse', 'scale_gradient', 'scattered_embedding_column', 'scattered_embedding_lookup', 'scattered_embedding_lookup_sparse', 'separable_conv2d', 'separable_convolution2d', 'sequence_input_from_feature_columns', 'shared_embedding_columns', 'six', 'softmax', 'sparse_column_with_hash_bucket', 'sparse_column_with_integerized_feature', 'sparse_column_with_keys', 'sparse_column_with_vocabulary_file', 'sparse_feature_cross', 'sparse_feature_cross_op', 'sparse_ops', 'sparse_tensor', 'sparse_tensor_py', 'spatial_softmax', 'stack', 'string_ops', 'sum_regularizer', 'summaries', 'summarize_activation', 'summarize_activations', 'summarize_biases', 'summarize_collection', 'summarize_tensor', 'summarize_tensors', 'summarize_variables', 'summarize_weights', 'summary', 'target_column', 'tensor_shape', 'tfexample_decoder', 'train', 'transform_features', 'unit_norm', 'utils', 'variable', 'variable_scope', 'variables', 'variance_scaling_initializer', 'vars_', 'vs', 'weighted_sparse_column', 'weighted_sum_from_feature_columns', 'xavier_initializer', 'xavier_initializer_conv2d', 'zero_initializer']\n",
      "OK  __main__ :> slim constant slim // ( -- obj ) google tf-slim \n",
      "OK .s\n",
      "empty\n",
      "\n",
      "OK exit\n",
      "OK "
     ]
    }
   ],
   "source": [
    "peforth.ok(loc=locals(), glo=globals(), cmd = '''\n",
    "    py> tos()[0] value locals\n",
    "    py> pop()[1] value globls\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從下面這段 code 看來，我從 [tensorflow-for-poets-2](https://github.com/hcchengithub/tensorflow-for-poets-2) 學來的 [AILAB 「剪刀、石頭、布」](https://github.com/hcchengithub/ailab_RockPaperScissors) 精簡很多。作者 Google codelabs [MarkDaoust](https://github.com/MarkDaoust) 的 `script/show_image.py` 把工作都做好了，直接帶我去應用。如今來讀 Kwot Sin 的文章，等於是帶我回頭理解前例的 source code.\n",
    "\n",
    "下面這些 dataset 相關的 code 會出 error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './labels.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-072af8fd1b3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#State the labels file and read it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mlabels_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./labels.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#Create a dictionary to refer each label to their string name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './labels.txt'"
     ]
    }
   ],
   "source": [
    "#================ DATASET INFORMATION ======================\n",
    "#State dataset directory where the tfrecord files are located\n",
    "dataset_dir = './data'  # was '.' 我覺得不好，改到 data 下\n",
    "\n",
    "#State where your log file is at. If it doesn't exist, create it.\n",
    "log_dir = './log'\n",
    "\n",
    "#State where your checkpoint file is\n",
    "checkpoint_file = './inception_resnet_v2_2016_08_30.ckpt'\n",
    "\n",
    "#State the image size you're resizing your images to. We will use the default inception size of 299.\n",
    "image_size = 299\n",
    "\n",
    "#State the number of classes to predict: 玩 inception v3、MobileNet 都不用管這個，可能是老師處理的\n",
    "num_classes = 5\n",
    "\n",
    "#State the labels file and read it\n",
    "labels_file = './labels.txt'\n",
    "labels = open(labels_file, 'r')\n",
    "\n",
    "#Create a dictionary to refer each label to their string name\n",
    "labels_to_name = {}\n",
    "for line in labels:\n",
    "    label, string_name = line.split(':')\n",
    "    string_name = string_name[:-1] #Remove newline\n",
    "    labels_to_name[int(label)] = string_name\n",
    "\n",
    "#Create the file pattern of your TFRecord files so that it could be recognized later on\n",
    "file_pattern = 'flowers_%s_*.tfrecord'\n",
    "\n",
    "#Create a dictionary that will help people understand your dataset better. This is required by the Dataset class later.\n",
    "items_to_descriptions = {\n",
    "    'image': 'A 3-channel RGB coloured flower image that is either tulips, sunflowers, roses, dandelion, or daisy.',\n",
    "    'label': 'A label that is as such -- 0:daisy, 1:dandelion, 2:roses, 3:sunflowers, 4:tulips'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================= TRAINING INFORMATION ==================\n",
    "#State the number of epochs to train\n",
    "num_epochs = 70\n",
    "\n",
    "#State your batch size\n",
    "batch_size = 10\n",
    "\n",
    "#Learning rate information and configuration (Up to you to experiment)\n",
    "initial_learning_rate = 0.0002\n",
    "learning_rate_decay_factor = 0.7\n",
    "num_epochs_before_decay = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First check whether the split_name is train or validation\n",
    "if split_name not in ['train', 'validation']:\n",
    "    raise ValueError('The split_name %s is not recognized.\\\n",
    "    Please input either train or validation as the split_name' % (split_name))\n",
    "\n",
    "#Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "file_pattern_path = os.path.join(dataset_dir, file_pattern % (split_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the total number of examples in all of these shard \n",
    "num_samples = 0\n",
    "file_pattern_for_counting = 'flowers_' + split_name\n",
    "tfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if\\                                                       file.startswith(file_pattern_for_counting)]\n",
    "for tfrecord_file in tfrecords_to_count:\n",
    "    for record in tf.python_io.tf_record_iterator(tfrecord_file):\n",
    "        num_samples += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(split_name, dataset_dir, file_pattern=file_pattern):\n",
    "    \"\"\"\n",
    "    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later\n",
    "    on. This function will set up the decoder and dataset information all into one Dataset class so that you can avoid\n",
    "    the brute work later on.\n",
    "    \n",
    "    Your file_pattern is very important in locating the files later. \n",
    "\n",
    "    INPUTS:\n",
    "        - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
    "        - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
    "        - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
    "\n",
    "    OUTPUTS:\n",
    "    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation.\n",
    "    \"\"\"\n",
    "    #First check whether the split_name is train or validation\n",
    "    if split_name not in ['train', 'validation']:\n",
    "        raise ValueError(\\\n",
    "        'The split_name %s is not recognized. Please input either train or validation as the split_name'\\\n",
    "        % (split_name))\n",
    "\n",
    "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "    file_pattern_path = os.path.join(dataset_dir, file_pattern % (split_name))\n",
    "\n",
    "    #Count the total number of examples in all of these shard\n",
    "    num_samples = 0\n",
    "    file_pattern_for_counting = 'flowers_' + split_name\n",
    "    tfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir)\\\n",
    "                         if file.startswith(file_pattern_for_counting)]\n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file):\n",
    "            num_samples += 1\n",
    "\n",
    "    #Create a reader, which must be a TFRecord reader in this case\n",
    "    reader = tf.TFRecordReader\n",
    "\n",
    "    #Create the keys_to_features dictionary for the decoder\n",
    "    keys_to_features = {\n",
    "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n",
    "      'image/class/label': tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "\n",
    "    #Create the items_to_handlers dictionary for the decoder.\n",
    "    items_to_handlers = {\n",
    "    'image': slim.tfexample_decoder.Image(),\n",
    "    'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "    }\n",
    "\n",
    "    #Start to create the decoder\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "    #Create the labels_to_name file\n",
    "    labels_to_name_dict = labels_to_name\n",
    "\n",
    "    #Actually create the dataset\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources = file_pattern_path,\n",
    "        decoder = decoder,\n",
    "        reader = reader,\n",
    "        num_readers = 4,\n",
    "        num_samples = num_samples,\n",
    "        num_classes = num_classes,\n",
    "        labels_to_name = labels_to_name_dict,\n",
    "        items_to_descriptions = items_to_descriptions)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
